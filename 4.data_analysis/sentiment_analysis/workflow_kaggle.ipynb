{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e1c8c",
   "metadata": {
    "_cell_guid": "5e178ad6-72d1-45dd-a32b-5f73cb50d0ec",
    "_uuid": "a3258195-88ae-44c0-88d7-52fc80b1f27f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-27T15:42:46.451891Z",
     "iopub.status.busy": "2022-06-27T15:42:46.450936Z",
     "iopub.status.idle": "2022-06-27T15:44:11.091105Z",
     "shell.execute_reply": "2022-06-27T15:44:11.090042Z"
    },
    "executionInfo": {
     "elapsed": 5033,
     "status": "ok",
     "timestamp": 1656275533572,
     "user": {
      "displayName": "Ondřej Pekáček",
      "userId": "15262765153837785656"
     },
     "user_tz": -120
    },
    "id": "Hc10Pywa0Huu",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ba25abea-096e-44d8-8e59-8cabb846df98",
    "papermill": {
     "duration": 84.654883,
     "end_time": "2022-06-27T15:44:11.093973",
     "exception": false,
     "start_time": "2022-06-27T15:42:46.439090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gdown only works reliably with individual files. For folders, limit is 50 files.\n",
    "# %conda install -y gdown\n",
    "%pip install pyreadr transformers torch datasets # --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b3033e",
   "metadata": {
    "_cell_guid": "c7c28722-9eb1-429b-a306-05573509b0a7",
    "_uuid": "7272ac55-020b-4fa2-9c9e-2c64b018e021",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-27T15:44:11.164772Z",
     "iopub.status.busy": "2022-06-27T15:44:11.163888Z",
     "iopub.status.idle": "2022-06-27T15:44:26.132023Z",
     "shell.execute_reply": "2022-06-27T15:44:26.130754Z"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1656278768380,
     "user": {
      "displayName": "Ondřej Pekáček",
      "userId": "15262765153837785656"
     },
     "user_tz": -120
    },
    "id": "ukaLqkpj0xzD",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.006167,
     "end_time": "2022-06-27T15:44:26.134591",
     "exception": false,
     "start_time": "2022-06-27T15:44:11.128424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from json import dump\n",
    "# import gdown\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "from torch import cuda\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from pyreadr import read_r\n",
    "from datasets import Sequence, Features, Value, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Specify path to input and output files\n",
    "PATH_TO_INPUT = \"../input/media_chunks/\"\n",
    "PATH_TO_OUTPUT = \"output/\"\n",
    "MODEL_PATH = \"model/\"\n",
    "MODEL_URL = \"https://air.kiv.zcu.cz/public/CZERT-B_fb.zip\"\n",
    "YEAR_FILTER = [\"2022\"]\n",
    "DEVICE = 0 if cuda.is_available() else -1\n",
    "BATCH_BY = 1\n",
    "# GDRIVE_DIR = UserSecretsClient().get_secret(\"GDRIVE_URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9067f",
   "metadata": {},
   "source": [
    "## Separate the sentiment analysis workflow into reusable functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_info():\n",
    "    \"\"\"Get the GPU information, if available.\"\"\"\n",
    "    if cuda.is_available():\n",
    "        print(\"GPU acceleration available\")\n",
    "        print(f\"\\nNr. of CUDA devices: {cuda.device_count()}\")\n",
    "        print(f\"\\nCurrent device nr.: {cuda.current_device()}\")\n",
    "        print(f\"\\nCurrent device name: {cuda.get_device_name(0)}\")\n",
    "        print(f\"\\nCurrent device properties: {cuda.get_device_properties(0)}\")\n",
    "        # Also, we could check for more details about the GPU:\n",
    "        print(os.system(\"nvidia-smi\"))\n",
    "    else:\n",
    "        print(\"GPU acceleration not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_and_output(input_dir: str, output_dir: str, gdrive_url: str = None, overwrite_input: bool = False, overwrite_output: bool = False) -> None:\n",
    "    \n",
    "    if not os.path.exists(input_dir) or overwrite_input:\n",
    "        os.makedirs(input_dir)\n",
    "    if not os.path.exists(output_dir) or overwrite_output:\n",
    "        os.makedirs(output_dir)\n",
    "    print(\"Input and output directories should be ready.\")\n",
    "    # Id from \"traditional\" sharing url google gives like https://drive.google.com/file/d/1xeA17aepaVk-dPz3dX8dbvAyQgoYhObR/view?usp=sharing\n",
    "    # file_id = re.search(\"(?<=file\\/d\\/)\\S{25,40}(?=\\/view)\", gdrive_url).group()\n",
    "    # url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    # Choose fuzzy if you want gdown to extract the file id itself.\n",
    "    # gdown.download(url, output, quiet=False, fuzzy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_download(model_path: str, model_url: str, overwrite_existing: bool = False) -> None:\n",
    "    \"\"\"Downloads selected model from specified url if it does not exist already.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model.\n",
    "        model_url (str): Url to the model.\n",
    "        overwrite_existing (bool, optional): Defaults to False.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path) or overwrite_existing:\n",
    "        os.system(f\"\"\"\n",
    "        mkdir model\n",
    "        wget {model_url} -O model/model.zip\n",
    "        unzip -j -d model/ model/model.zip\n",
    "        rm model/model.zip\n",
    "        \"\"\")\n",
    "        print(f\"Model downloaded to folder {model_path}\")\n",
    "    else:\n",
    "        print(\"Model already downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dceddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_analytical_pipeline(model_path: str, processing_device: int):\n",
    "    \"\"\"Configures the pipeline for the sentiment analysis using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the model.\n",
    "        processing_device (int): Device to use for processing. 0 for GPU, -1 for CPU.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Pipeline\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_path,\n",
    "        model_max_length=512)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_path)\n",
    "    return pipeline(\"sentiment-analysis\",\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    device=processing_device,\n",
    "                    max_length=512,\n",
    "                    padding=\"longest\",\n",
    "                    truncation=True) # Current version of transformers at Kaggle does not support top_k=n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651cd1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_new_files(path_to_input: str, path_to_output: str) -> list:\n",
    "    \"\"\"Get all files in the input directory that are not already in the output directory.\n",
    "\n",
    "    Args:\n",
    "        path_to_input (str): Path to the input directory.\n",
    "        path_to_output (str): Path to the output directory.\n",
    "\n",
    "    Returns:\n",
    "        list: List of files in the input directory that are not already in the output directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_to_output):\n",
    "        # create directory if it doesn't exist\n",
    "        os.makedirs(path_to_output)\n",
    "    existing_processed_files = {Path(file.replace(\"sentiment_\", \"\")).stem for file in os.listdir(\n",
    "        path_to_output) if file.endswith((\".rds\", \".json\"))}\n",
    "    return sorted(({Path(file).stem for file in os.listdir(\n",
    "        path_to_input) if file.endswith(\".rds\")} - existing_processed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e59c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_years(input_files: list, year_filter: list) -> tuple:\n",
    "    \"\"\"Filter the files by the specified years.\n",
    "\n",
    "    Args:\n",
    "        input_files (list): List of files in the input directory.\n",
    "        year_filter (list): List of years to filter by.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of filtered files.\n",
    "    \"\"\"\n",
    "    return tuple(file for file in input_files if any(\n",
    "        year in file for year in year_filter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60aa981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer loop over the files in the input directory.\n",
    "def sentiment_analysis_workflow(path_to_input: str,\n",
    "                                path_to_output: str,\n",
    "                                model_pipeline,\n",
    "                                input_files_filtered: tuple,\n",
    "                                batch_by: int = 1) -> None:\n",
    "    \"\"\"Performs the sentiment analysis workflow on selected files from the input directory.\n",
    "\n",
    "    Args:\n",
    "        path_to_input (str): Path to the input directory.\n",
    "        path_to_output (str): Path to the output directory.\n",
    "        model_pipeline (_type_): Pipeline for the sentiment analysis.\n",
    "        input_files_filtered (tuple): Tuple of filtered files.\n",
    "        batch_by (int, optional): Size of the batch send to the model pipeline. Defaults to 1.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Starting sentiment analysis workflow on {len(input_files_filtered)} files.\")\n",
    "    for count, file in enumerate(input_files_filtered, start=1):\n",
    "        # Reading one chunk from the input directory\n",
    "        udpipe_chunk = read_r(f\"{path_to_input + file}.rds\")[None][[\"doc_id\", \"sentence_id\", \"token\"]] \\\n",
    "            .groupby(['doc_id', \"sentence_id\"], sort=False, as_index=False) \\\n",
    "            .agg(\n",
    "                tokens=(\"token\", \" \".join)) \\\n",
    "            .groupby(['doc_id'], sort=False, as_index=False) \\\n",
    "            .agg(\n",
    "                text=(\"tokens\", list)\n",
    "        )\n",
    "            \n",
    "        udpipe_chunk = Dataset.from_pandas(udpipe_chunk, features=Features(\n",
    "            {\"doc_id\": Value(dtype=\"string\", id=None),\n",
    "             \"text\": Sequence(feature=Value(dtype='string', id=None),\n",
    "                              length=-1, id=None)}))\n",
    "\n",
    "        print(\n",
    "            f\"\"\"Starting sentinment analysis for chunk {file}.\n",
    "            File contains {udpipe_chunk.shape[0]} articles.\"\"\", flush=True)\n",
    "        # Inner loop over the individual texts in the dataframe. We can use batching,\n",
    "        # but for longer text sizes, it does not seem to make much (if any) difference.\n",
    "        sentiment_dict = {document[\"doc_id\"]: model_pipeline(\n",
    "            document[\"text\"], batch_size=batch_by) for document in tqdm(udpipe_chunk)}\n",
    "        \n",
    "        with open(file=f\"{path_to_output}sentiment_{file}.json\", mode=\"w\", encoding=\"utf8\") as out:\n",
    "            dump(sentiment_dict, out, sort_keys=False)\n",
    "        \n",
    "        print(\n",
    "            f\"Finished the sentiment analysis of the chunk {file}, nr. {count} out of {len(input_files_filtered)}.\",\n",
    "            flush=True)\n",
    "        print('All changes made in this Kaggle session should now be visible locally.', flush=True)\n",
    "# End of outer loop.\n",
    "    print(\"Finished the sentiment analysis of all chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc6ab1",
   "metadata": {},
   "source": [
    "## Run specified functions with selected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if GPU availabe\n",
    "get_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c0f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_input_and_output(input_dir=PATH_TO_INPUT, output_dir=PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19116b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model, run once\n",
    "model_download(\n",
    "    model_path=MODEL_PATH,\n",
    "    model_url=MODEL_URL,\n",
    "    overwrite_existing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pipeline based on whether GPU is available. 0 and higher are CUDA devices and -1 is CPU\n",
    "model_configured = configure_analytical_pipeline(\n",
    "    model_path=MODEL_PATH, processing_device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "udpipe_files = get_only_new_files(\n",
    "    path_to_input=PATH_TO_INPUT, path_to_output=PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d45062",
   "metadata": {},
   "outputs": [],
   "source": [
    "udpipe_files_filtered = filter_by_years(\n",
    "    input_files=udpipe_files, year_filter=YEAR_FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "sentiment_analysis_workflow(\n",
    "    path_to_input=PATH_TO_INPUT,\n",
    "    path_to_output=PATH_TO_OUTPUT,\n",
    "    model_pipeline=model_configured,\n",
    "    input_files_filtered=udpipe_files_filtered,\n",
    "    batch_by=BATCH_BY)\n",
    "print(f\"Finished sentiment analysis in {time() - start_time} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 136.634654,
   "end_time": "2022-06-27T15:44:53.495185",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-27T15:42:36.860531",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
