{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc10Pywa0Huu",
        "outputId": "277c3fe1-fbb0-49a1-dc7b-28547050c397"
      },
      "outputs": [],
      "source": [
        "%pip install pyreadr transformers torch datasets # --extra-index-url https://download.pytorch.org/whl/cu113"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukaLqkpj0xzD"
      },
      "outputs": [],
      "source": [
        "\"\"\"Import necessary modules.\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from json import dump\n",
        "from torch import cuda\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from pyreadr import read_r\n",
        "from datasets import Sequence, Features, Value, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# Specify path to input and output files\n",
        "PATH_TO_INPUT = \"drive/MyDrive/media_research/datasets/2.processed_data/udpipe_processed/chunks/\"\n",
        "PATH_TO_OUTPUT = \"drive/MyDrive/media_research/datasets/4.analyzed_data/media/sentiment/chunks/\"\n",
        "MODEL_PATH = \"model/\"\n",
        "MODEL_URL = \"https://air.kiv.zcu.cz/public/CZERT-B_fb.zip\"\n",
        "YEAR_FILTER = [\"2015\"]\n",
        "DEVICE = 0 if cuda.is_available() else -1\n",
        "BATCH_BY = 1\n",
        "\n",
        "# Mount GDrive storage.\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Separate the sentiment analysis workflow into reusable functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_gpu_info():\n",
        "    \"\"\"Get the GPU information, if available.\"\"\"\n",
        "    if cuda.is_available():\n",
        "        print(\"GPU acceleration available\")\n",
        "        print(f\"\\nNr. of CUDA devices: {cuda.device_count()}\")\n",
        "        print(f\"\\nCurrent device nr.: {cuda.current_device()}\")\n",
        "        print(f\"\\nCurrent device name: {cuda.get_device_name(0)}\")\n",
        "        print(f\"\\nCurrent device properties: {cuda.get_device_properties(0)}\")\n",
        "        # Also, we could check for more details about the GPU:\n",
        "        print(os.system(\"nvidia-smi\"))\n",
        "    else:\n",
        "        print(\"GPU acceleration not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_download(model_path: str, model_url: str, overwrite_existing: bool = False) -> None:\n",
        "    \"\"\"Downloads selected model from specified url if it does not exist already.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the model.\n",
        "        model_url (str): Url to the model.\n",
        "        overwrite_existing (bool, optional): Defaults to False.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(model_path) or overwrite_existing:\n",
        "        os.system(f\"\"\"\n",
        "        mkdir model\n",
        "        wget {model_url} -O model/model.zip\n",
        "        unzip -j -d model/ model/model.zip\n",
        "        rm model/model.zip\n",
        "        \"\"\")\n",
        "        print(f\"Model downloaded to folder {model_path}\")\n",
        "    else:\n",
        "        print(\"Model already downloaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6cRBjA43sHf"
      },
      "outputs": [],
      "source": [
        "def configure_analytical_pipeline(model_path: str, processing_device: int):\n",
        "    \"\"\"Configures the pipeline for the sentiment analysis using the specified model.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the model.\n",
        "        processing_device (int): Device to use for processing. 0 for GPU, -1 for CPU.\n",
        "\n",
        "    Returns:\n",
        "        _type_: Pipeline\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path=model_path,\n",
        "        model_max_length=512)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        pretrained_model_name_or_path=model_path)\n",
        "    return pipeline(\"sentiment-analysis\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=processing_device,\n",
        "                    max_length=512,\n",
        "                    padding=\"longest\",\n",
        "                    truncation=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_only_new_files(path_to_input: str, path_to_output: str) -> list:\n",
        "    \"\"\"Get all files in the input directory that are not already in the output directory.\n",
        "\n",
        "    Args:\n",
        "        path_to_input (str): Path to the input directory.\n",
        "        path_to_output (str): Path to the output directory.\n",
        "\n",
        "    Returns:\n",
        "        list: List of files in the input directory that are not already in the output directory.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path_to_output):\n",
        "        # create directory if it doesn't exist\n",
        "        os.makedirs(path_to_output)\n",
        "    existing_processed_files = {Path(file.replace(\"sentiment_\", \"\")).stem for file in os.listdir(\n",
        "        path_to_output) if file.endswith((\".rds\", \".json\"))}\n",
        "    return sorted(({Path(file).stem for file in os.listdir(\n",
        "        path_to_input) if file.endswith(\".rds\")} - existing_processed_files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_years(input_files: list, year_filter: list) -> tuple:\n",
        "    \"\"\"Filter the files by the specified years.\n",
        "\n",
        "    Args:\n",
        "        input_files (list): List of files in the input directory.\n",
        "        year_filter (list): List of years to filter by.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple of filtered files.\n",
        "    \"\"\"\n",
        "    return tuple(file for file in input_files if any(\n",
        "        year in file for year in year_filter))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outer loop over the files in the input directory.\n",
        "def sentiment_analysis_workflow(path_to_input: str,\n",
        "                                path_to_output: str,\n",
        "                                model_pipeline,\n",
        "                                input_files_filtered: tuple,\n",
        "                                batch_by: int = 1) -> None:\n",
        "    \"\"\"Performs the sentiment analysis workflow on selected files from the input directory.\n",
        "\n",
        "    Args:\n",
        "        path_to_input (str): Path to the input directory.\n",
        "        path_to_output (str): Path to the output directory.\n",
        "        model_pipeline (_type_): Pipeline for the sentiment analysis.\n",
        "        input_files_filtered (tuple): Tuple of filtered files.\n",
        "        batch_by (int, optional): Size of the batch send to the model pipeline. Defaults to 1.\n",
        "    \"\"\"\n",
        "    print(\n",
        "        f\"Starting sentiment analysis workflow on {len(input_files_filtered)} files.\")\n",
        "    for count, file in enumerate(input_files_filtered, start=1):\n",
        "        # Mount GDrive storage in each iteration.\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        # Reading one chunk from the input directory\n",
        "        udpipe_chunk = read_r(f\"{path_to_input + file}.rds\")[None][[\"doc_id\", \"sentence_id\", \"token\"]] \\\n",
        "            .groupby(['doc_id', \"sentence_id\"], sort=False, as_index=False) \\\n",
        "            .agg(\n",
        "                tokens=(\"token\", \" \".join)) \\\n",
        "            .groupby(['doc_id'], sort=False, as_index=False) \\\n",
        "            .agg(\n",
        "                text=(\"tokens\", list)\n",
        "        )\n",
        "            \n",
        "        udpipe_chunk = Dataset.from_pandas(udpipe_chunk, features=Features(\n",
        "            {\"doc_id\": Value(dtype=\"string\", id=None),\n",
        "             \"text\": Sequence(feature=Value(dtype='string', id=None),\n",
        "                              length=-1, id=None)}))\n",
        "\n",
        "        print(\n",
        "            f\"\"\"Starting sentinment analysis for chunk {file}.\n",
        "            File contains {udpipe_chunk.shape[0]} articles.\"\"\", flush=True)\n",
        "        # Inner loop over the individual texts in the dataframe. We can use batching,\n",
        "        # but for longer text sizes, it does not seem to make much (if any) difference.\n",
        "        sentiment_dict = {document[\"doc_id\"]: model_pipeline(\n",
        "            document[\"text\"], batch_size=batch_by) for document in tqdm(udpipe_chunk)}\n",
        "        \n",
        "        with open(file=f\"{path_to_output}sentiment_{file}.json\", mode=\"w\", encoding=\"utf8\") as out:\n",
        "            dump(sentiment_dict, out, sort_keys=False)\n",
        "        \n",
        "        print(\n",
        "            f\"Finished the sentiment analysis of the chunk {file}, nr. {count} out of {len(input_files_filtered)}.\",\n",
        "            flush=True)\n",
        "        drive.flush_and_unmount()\n",
        "        print('All changes made in this Colab session should now be visible in Drive.', flush=True)\n",
        "# End of outer loop.\n",
        "    print(\"Finished the sentiment analysis of all chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run specified functions with selected parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect if GPU availabe\n",
        "get_gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the model, run once\n",
        "model_download(\n",
        "    model_path=MODEL_PATH,\n",
        "    model_url=MODEL_URL,\n",
        "    overwrite_existing=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Choose pipeline based on whether GPU is available. 0 and higher are CUDA devices and -1 is CPU\n",
        "model_configured = configure_analytical_pipeline(\n",
        "    model_path=MODEL_PATH, processing_device=DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "udpipe_files = get_only_new_files(\n",
        "    path_to_input=PATH_TO_INPUT, path_to_output=PATH_TO_OUTPUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "udpipe_files_filtered = filter_by_years(\n",
        "    input_files=udpipe_files, year_filter=YEAR_FILTER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "sentiment_analysis_workflow(\n",
        "    path_to_input=PATH_TO_INPUT,\n",
        "    path_to_output=PATH_TO_OUTPUT,\n",
        "    model_pipeline=model_configured,\n",
        "    input_files_filtered=udpipe_files_filtered,\n",
        "    batch_by=BATCH_BY)\n",
        "print(f\"Finished sentiment analysis in {time() - start_time} seconds.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "czech_sent_class.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 ('datascience')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "7fcc423611767790f2393ecee3bb6574101f9de8433eefd3b59182ef24a3412e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
