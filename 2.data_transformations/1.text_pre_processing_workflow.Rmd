---
title: "Text Pre-processing"
---
Load necessary packages
```{r}
# Package names
packages <- c("dplyr", "stringr", "purrr", "tidyr")

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

Use Python to process the full media article chunks with regex, saving all of them
```{python}
import os
import gc
from pathlib import Path
from pyreadr import read_r, write_rds
from tqdm import tqdm
from media_articles.cs_text_pre_process import pattern_preprocessing_cs

INPUT_DIR = "../1.data_sources/media_articles/data/full/chunks/"
OUTPUT_DIR = "media_articles/data/regex_processed/chunks/"
PATTERN = r"<(.|\n)*?>|[^ěščřžýáíéóúůďťňĎŇŤŠČŘŽÝÁÍÉÚŮĚÓa-zA-Z0-9.?! ]"

if not os.path.exists(OUTPUT_DIR):
      # create directory if it doesn't exist
      os.makedirs(OUTPUT_DIR)
      
# Check for already processed files in the output directory
existing_processed_files = {Path(file.replace("regex_", "")).stem for file in os.listdir(
        OUTPUT_DIR) if file.endswith(".rds")}
        
# Get names of all of the chunks to be processed        
chunks = sorted(({Path(file).stem for file in os.listdir(INPUT_DIR) if file.endswith(".rds")} - existing_processed_files))

print(f"""The following chunks will will be processed: \n {chunks}""")

gc.enable()

## We can read all of the chunks into one large dataset. However, the further processing might prove to be to memory demanding.
# media_list_of_df = (read_r(file)[None][["Code", "PublishDate", "Title", "Content"]] \
#                     .rename(columns = {"Code":"article_id", "PublishDate":"date", "Title":"title", "Content":"text"}) for file in chunks_paths)
# media_df = pd.concat(media_list_of_df, ignore_index=True)

# Loop over the list of .rds chunks, load each one in, process it and save it to a different folder
start = time()
for chunk in tqdm(chunks):
    media_df = read_r(f"{INPUT_DIR}{chunk}.rds")[None][["Code", "Title", "Content"]] \
        .rename(columns={"Code": "article_id", "Title": "title", "Content": "text"})
    media_df["text"] = media_df["title"] + ". " + media_df["text"]
    media_df.drop("title", axis=1, inplace=True)
    media_df["text"] = pattern_preprocessing_cs(input_column=media_df["text"],
                                        pattern=PATTERN)
    write_rds(f"{OUTPUT_DIR}regex_{chunk}.rds", media_df, compress="gzip")
    del media_df
    gc.collect()
print("Regex preprocessing finished")
print("End in:", time() - start)

```

# Alternative workflow with Polars library
```{python}
import polars as pl
PATTERN = r"<(.|\n)*?>|[^ěščřžýáíéóúůďťňĎŇŤŠČŘŽÝÁÍÉÚŮĚÓa-zA-Z0-9.?! ]"
for chunk in tqdm(chunks):
    media_df = pl.read_ipc(f"{INPUT_DIR}{chunk}.feather") \
                .lazy() \
                .rename({"Code": "article_id", "Title": "title", "Content": "text"}) \
                .with_columns([
                    (pl.col("title") + ". " + pl.col("text"))
                    .str.replace_all(PATTERN, " ")
                    .str.replace_all(r"\.{2,}", ".")
                    .str.strip()
                    .str.replace_all(r"  +", " ")
                    .alias("text")]) \
                .select(["article_id", "text"]) \
                .collect() \
                .write_ipc(f"{OUTPUT_DIR}regex_{chunk}.feather", compression = "zstd")
    del media_df
    gc.collect()
print("Regex preprocessing finished")
```



Separate large chunks to smaller parts for easier sharing and processing

```{r}
list_of_processed_chunks <- list.files(path = "media_articles/data/regex_processed/chunks", pattern = "*.rds", full.names = TRUE)

for (i in list_of_processed_chunks) {
  if (file.size(i) > 1) {
    full_df <- readRDS(i)

    no_of_chunks <- ceiling(nrow(full_df) / 3500) # 3500 is the desired max number of rows

    chunk_nr_index <- ceiling(1:nrow(full_df) / nrow(full_df) * no_of_chunks)

    list_of_chunks <- split(full_df, chunk_nr_index)

    lapply(names(list_of_chunks), function(df) {
      saveRDS(list_of_chunks[[df]], file = paste0(substr(i, 1, nchar(i) - 4), "_part_", df, ".rds"))
    })

    unlink(i)
  }
}

# purrr::map2(list_of_chunks, paste0("media_articles/data/df_chunks/non_processed/cs_chunk_", names(list_of_chunks), ".rds"), saveRDS)
```

Paralellized version (works on Linux)
```{r}

list_of_processed_chunks <- list.files(path = "media_articles/data/regex_processed/chunks", pattern = "*.rds", full.names = TRUE)

library(parallel)

process_df <- function(i) {
  full_df <- readRDS(i)

  no_of_chunks <- ceiling(nrow(full_df) / 3500) # 3500 is the desired max number of rows

  chunk_nr_index <- ceiling(1:nrow(full_df) / nrow(full_df) * no_of_chunks)

  list_of_chunks <- split(full_df, chunk_nr_index)

  lapply(names(list_of_chunks), function(df) {
    saveRDS(list_of_chunks[[df]], file = paste0(substr(i, 1, nchar(i) - 4), "_part_", df, ".rds"))
  })

  unlink(i)
}

mclapply(list_of_processed_chunks, process_df, mc.cores = detectCores() - 1)
```

# Processing regex-cleaned chunks with UDPIPE model (via API)
# Using RStudio jobs to parallelize 
```{r}
# Set up parameters for this job
udpipe_period <- "2022"
udpipe_exclude <- FALSE

setwd("media_articles")

# Identify dataset chunks of interest
all_chunks_path_udpipe <- list.files(path = file.path("data", "regex_processed", "chunks"), pattern = "*.rds", full.names = TRUE) %>%
  .[grep(pattern = udpipe_period, ., invert = udpipe_exclude)] %>%
  sort() %>%
  .[2:37]

all_chunks_name_udpipe <- list.files(path = file.path("data", "regex_processed", "chunks"), pattern = "*.rds", full.names = FALSE) %>%
  .[grep(pattern = udpipe_period, ., invert = udpipe_exclude)] %>%
  sort() %>%
  .[2:37]

# Run the script as a RStudio job
rstudioapi::jobRunScript(path = "udpipe_rstudio_jobs.R", importEnv = TRUE)
```

Integrity check: Which years have we processed with UDPIPE 
```{r}
# Read the UDPIPE processed chunks back in, we only the id and date
udpipe_df_ids <- list.files(path = "media_articles/data/udpipe_processed", pattern = "*.rds", full.names = TRUE) %>%
  map_dfr(readRDS) %>%
  pull(doc_id) %>%
  unique() # keep only unique ids

# Check which ids are collected for the specific period
full_articles_selected_period <- list.files(path = "media_articles/data/regex_processed/chunks", pattern = "*.rds", full.names = TRUE) %>%
  .[grep("2015", .)] %>%
  map_dfr(readRDS, .id = "filename")

full_articles_dates <- list.files(path = "../1.data_sources/media_articles/data/full/chunks/", pattern = "*.rds", full.names = TRUE) %>%
  map_dfr(readRDS) %>%
  select(PublishDate, Code)

filtered_dates <- full_articles_dates[full_articles_dates$Code %in% udpipe_df_ids, ]

summary(filtered_dates$PublishDate)

# Function to compare the two id vectors
xtab_set <- function(A, B) {
  both <- union(A, B)
  inA <- both %in% A
  inB <- both %in% B
  return(table(inA, inB))
}

xtab_set(udpipe_df_ids, udpipe_df_ids_new)
```

